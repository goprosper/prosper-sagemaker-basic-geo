{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker as sm\n",
    "\n",
    "# Specify the ARN of the model package you will be using. You can get this\n",
    "# from the sagemaker console after you subscribe to the model package.\n",
    "model_package_arn = 'arn:aws:sagemaker:us-east-2:214666064132:model-package/use-uber-reg-basic-geo'\n",
    "\n",
    "# Load the zip code dictionary used to prepare the data\n",
    "\n",
    "zip_dict = {}\n",
    "    \n",
    "import csv\n",
    "with open('data/zip_features.csv', newline='') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    \n",
    "    # skip the header\n",
    "    next(csvreader)\n",
    "\n",
    "    for row in csvreader:\n",
    "        zip_dict[int(row[0])] = (int(row[1]), int(row[2]))  # (cluster, division)\n",
    "        \n",
    "    \n",
    "def get_zip_feature_string(zip_code):\n",
    "    \n",
    "    # Retrieve cluster and division from dictionary. \n",
    "    # If zip is not found, return all zeros.\n",
    "    try:\n",
    "        (cluster, division) = zip_dict[zip_code]\n",
    "    except KeyError:\n",
    "        return \"0\" * 25\n",
    "    \n",
    "    cluster_list = ['0'] * 16  # initialize to 16 zeros\n",
    "    cluster_list[cluster] = '1'\n",
    "    \n",
    "    division_list = ['0'] * 9  # initialize to 9 zeros\n",
    "    division_list[division] = '1'\n",
    "    \n",
    "    return f'{\",\".join(division_list)},{\",\".join(cluster_list)}'\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0\n"
     ]
    }
   ],
   "source": [
    "print(get_zip_feature_string(43017))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realtime Inference Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy model to endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:sagemaker:us-east-2:214666064132:model-package/use-uber-reg-basic-geo\n",
      "prosper-test-endpoint\n",
      "-------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Set endpoint name. You may replace the following with any name you like. \n",
    "endpoint_name = 'prosper-test-endpoint-' + time.strftime(\"%Y%m%d%H%M%S\", time.localtime())  \n",
    "\n",
    "# Create model from model package\n",
    "print(model_package_arn)\n",
    "print(endpoint_name)\n",
    "model = sm.ModelPackage(\n",
    "            role=sm.get_execution_role(),\n",
    "            model_package_arn=model_package_arn,\n",
    "            sagemaker_session=sm.Session())\n",
    "\n",
    "# Deploy the model to an endpoint. Be sure to delete the endpoint when you are finished with it.\n",
    "# By default, this method waits until the endpoint is deployed. This could take a while.\n",
    "# To have the API return immediately, set the wait parameter to false.\n",
    "model.deploy(1, 'ml.m4.xlarge', endpoint_name=endpoint_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use endpoint created above to do realtime prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request Data: 1,5,24,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0\n",
      "Probability: 0.4339768588542938\n"
     ]
    }
   ],
   "source": [
    "# create predictor for endpoint created above\n",
    "predictor = sm.predictor.RealTimePredictor(endpoint_name, content_type='text/csv')\n",
    "\n",
    "# Sample request\n",
    "# Male\n",
    "gender = 1\n",
    "# Age Range: 55-64\n",
    "age_range = 5\n",
    "# Household Income: $150,000+\n",
    "household_income_range = 24\n",
    "# zip features: 43017\n",
    "zip_features = get_zip_feature_string(43017)\n",
    "\n",
    "# format request data as comma-delimited string\n",
    "request_data = f'{gender},{age_range},{household_income_range},{zip_features}'\n",
    "print(f'Request Data: {request_data}')\n",
    "\n",
    "# Submit the request to the endpoint.\n",
    "# By default, the result is returned as a sequence of bytes. We decode it as utf-8 string.\n",
    "# Note that there are parameters available for serializing and deserializing input and output data.\n",
    "result = predictor.predict(request_data).decode('utf-8')\n",
    "\n",
    "# print the result. This is the probability that the person with the requested parameters are in the\n",
    "# target class.\n",
    "print(f'Probability: {result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.session.Session().delete_endpoint(endpoint_name)\n",
    "sm.session.Session().delete_endpoint_config(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Whole File with Batch Transform Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform input: s3://sagemaker-us-east-2-214666064132/prosper-sample-data/basic-test-data/basic_test_data.csv\n"
     ]
    }
   ],
   "source": [
    "# A sample input file (data/basic_test_data.csv) is provided with this notebook.\n",
    "# The transform job expects its input file to to live in S3.  We upload the file\n",
    "# to the default bucket with key_prefix of prosper-sample-data/basic-test-data. You\n",
    "# can upload to any bucket and key you like if you specify the bucket and key_prefix parameters.\n",
    "\n",
    "transform_input = sm.Session().upload_data('data/basic_test_data.csv', key_prefix='prosper-sample-data/basic-test-data')\n",
    "print(f'transform input: {transform_input}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the transform job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform output: s3://sagemaker-us-east-2-214666064132/trends-styles-important-basic-2019-11-2-2019-11-22-18-17-23-276\n"
     ]
    }
   ],
   "source": [
    "# Create model from model package\n",
    "model = sm.ModelPackage(\n",
    "            role=sm.get_execution_role(),\n",
    "            model_package_arn=model_package_arn,\n",
    "            sagemaker_session=sm.Session())\n",
    "\n",
    "# Create the transformer\n",
    "# A variety of parameters may be specified here including the output path where\n",
    "# SageMaker will send the results of the transform. Since we do not specify the output,\n",
    "# Sagemaker will leave the results in the default bucket. We will retrieve this location \n",
    "# below so that we can inspect the output.\n",
    "transformer = model.transformer(1, 'ml.m4.xlarge')\n",
    "\n",
    "# Run the transform job.\n",
    "# By default, the output file contains only the inference result for each row.\n",
    "# You can use the output_filter parameter to include any of the input columns. Review also\n",
    "# input_filter which allows you to filter the parameters passed as input to the transformer.\n",
    "# The combination of input_filter and output_filter gives you a lot of flexibility.\n",
    "# By default, the API does not wait for the transform job to complete. You can control this with\n",
    "# the wait parameter.\n",
    "transformer.transform(transform_input, content_type='text/csv')\n",
    "\n",
    "# The transform job sets the output path in the output_path member.\n",
    "print(f'Transform output: {transformer.output_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
